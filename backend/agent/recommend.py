import os
from typing import Dict, List
from openai import OpenAI
from dotenv import load_dotenv
import asyncio
from agent.weather_agent import WeatherAgent

load_dotenv()
RECOMMEND_AGENT_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªç©¿è¡£æ¨èæ™ºèƒ½ä½“ï¼Œæ“…é•¿æ ¹æ®å¤©æ°”ä¿¡æ¯å’Œç”¨æˆ·éœ€æ±‚ï¼Œç»™å‡ºå®ç”¨ã€åˆç†çš„ç©¿è¡£å»ºè®®ã€‚

ä½ å°†æ¥æ”¶ä¸¤ç±»è¾“å…¥ä¿¡æ¯ï¼š
1. å¤©æ°”æ•°æ®ï¼ˆJSON æ ¼å¼ï¼‰ï¼Œå¯èƒ½åŒ…å«ï¼š
   - æ—¥æœŸï¼ˆdateï¼‰
   - ç™½å¤©/å¤œé—´å¤©æ°”çŠ¶å†µï¼ˆdayweather, nightweatherï¼‰
   - ç™½å¤©/å¤œé—´æ¸©åº¦ï¼ˆdaytemp, nighttempï¼‰
   - é£å‘ã€é£åŠ›ï¼ˆdaywind, daypowerï¼‰
2. ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
- æ­£ç¡®ç†è§£å¤©æ°”ä¿¡æ¯ï¼Œä¸è¦è‡†é€ ä¸å­˜åœ¨çš„æ•°æ®
- é‡ç‚¹å…³æ³¨ä»¥ä¸‹ç©¿è¡£ç›¸å…³å› ç´ ï¼š
  - æ°”æ¸©é«˜ä½ï¼ˆå°¤å…¶æ˜¯æœ€ä½/æœ€é«˜æ¸©ï¼‰
  - æ˜¼å¤œæ¸©å·®
  - å¤©æ°”çŠ¶å†µï¼ˆæ™´ã€é›¨ã€é›ªã€å¤§é£ç­‰ï¼‰
- ç»“åˆç”¨æˆ·æŸ¥è¯¢è¯­å¢ƒï¼ˆå¦‚æ˜¯å¦å‡ºè¡Œã€æ˜¯å¦æ—©æ™šæ´»åŠ¨ï¼‰
- ç»™å‡ºã€æ¸…æ™°ã€å…·ä½“ã€å¯æ‰§è¡Œã€‘çš„ç©¿è¡£å»ºè®®

è¾“å‡ºè¦æ±‚ï¼š
- ä½¿ç”¨è‡ªç„¶ã€å‹å¥½çš„ä¸­æ–‡
- ç›´æ¥ç»™å‡ºå»ºè®®ï¼Œä¸è¦è§£é‡Šä½ çš„æ¨ç†è¿‡ç¨‹
- ä¸è¦è¾“å‡º JSONï¼Œä¸è¦æåŠâ€œæ ¹æ®æ•°æ®â€â€œç³»ç»Ÿæç¤ºâ€ç­‰å†…éƒ¨ä¿¡æ¯
- å¦‚æœå¤©æ°”è¾ƒå†·ï¼Œæ˜ç¡®æŒ‡å‡ºå¤–å¥—ç±»å‹ï¼ˆå¦‚ï¼šç¾½ç»’æœã€æ£‰æœã€åšå¤–å¥—ï¼‰
- å¦‚æœæ¸©å·®è¾ƒå¤§ï¼Œæé†’â€œæ—©æ™šæ³¨æ„ä¿æš–â€
- å¦‚æœ‰å¿…è¦ï¼Œå¯é™„å¸¦ç®€å•çš„ç”Ÿæ´»å»ºè®®ï¼ˆå¦‚é˜²é£ã€é˜²æ™’ï¼‰

è¯·æ ¹æ®å®é™…è¾“å…¥çµæ´»ç”Ÿæˆå›ç­”ã€‚
"""

class RecommendAgent:
    def __init__(self):
        print("ğŸ”„ å¼€å§‹åˆå§‹æ¨èagent...")
        self.client = OpenAI(
            api_key=os.getenv("LLM_API_KEY"),
            base_url=os.getenv("LLM_BASE_URL")
        )
        self.weather_agent = WeatherAgent()

    def _build_query(self, weather_data, user_input):
        query = f"""
        ä½ å°†æ¥æ”¶åˆ°ä¸¤éƒ¨åˆ†ä¿¡æ¯ï¼š
        ã€å¤©æ°”æ•°æ®ã€‘ï¼š
        {weather_data}

        ã€ç”¨æˆ·éœ€æ±‚ã€‘ï¼š
        {user_input}

        è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›ã€å…·ä½“ã€å®ç”¨çš„ç©¿è¡£å»ºè®®ã€‘ã€‚

        è¦æ±‚ï¼š
        1. é‡ç‚¹è€ƒè™‘æ°”æ¸©é«˜ä½ã€æ˜¼å¤œæ¸©å·®ã€å¤©æ°”çŠ¶å†µï¼ˆå¦‚æ™´ã€é›¨ã€å¤§é£ç­‰ï¼‰
        2. æ˜ç¡®ç»™å‡ºå¤–å¥—ç±»å‹ã€å†…æ­å»ºè®®ï¼ˆå¦‚ç¾½ç»’æœ/åšå¤–å¥—/æ¯›è¡£ç­‰ï¼‰
        3. å¦‚æ˜¼å¤œæ¸©å·®è¾ƒå¤§ï¼Œè¯·æé†’æ—©æ™šæ³¨æ„ä¿æš–
        4. ä¸è¦è¾“å‡º JSONã€ä¸è¦è§£é‡Šæ¨ç†è¿‡ç¨‹
        """

        return query

    def call_llm_api_stream(self, messages: List[Dict], max_retries: int = 3):
        for attempt in range(max_retries):
            try:
                stream = self.client.chat.completions.create(
                    model=os.getenv("LLM_MODEL_ID"),
                    messages=messages,
                    temperature=0.3,
                    max_tokens=2048,
                    stream=True
                )

                for chunk in stream:
                    delta = chunk.choices[0].delta
                    if delta and delta.content:
                        yield delta.content

                return
            except Exception as e:
                if attempt == max_retries - 1:
                    raise e

    def run(self, user_input):
        weather_data = self.weather_agent.run(user_input)
        if not weather_data["success"]:
            return "æˆ‘åªèƒ½æŸ¥è¯¢æœªæ¥4å¤©çš„å¤©æ°”"
        query = self._build_query(weather_data, user_input)

        messages = [
            {"role": "system", "content": RECOMMEND_AGENT_PROMPT},
            {"role": "user", "content": query}
        ]

        full_response = ""
        for chunk in self.call_llm_api_stream(messages):
            print(chunk, end="", flush=True)   # ğŸ‘ˆ å®æ—¶è¾“å‡º
            full_response += chunk

        print()  # æ¢è¡Œ
        return full_response

    async def run_stream(self, user_input: str):
        """å¼‚æ­¥æµå¼è¿”å›æ¨èå†…å®¹"""
        weather_data = await asyncio.to_thread(self.weather_agent.run, user_input)
        if not weather_data["success"]:
            yield "æˆ‘åªèƒ½æŸ¥è¯¢æœªæ¥4å¤©çš„å¤©æ°”"
            return

        query = self._build_query(weather_data, user_input)
        messages = [
            {"role": "system", "content": RECOMMEND_AGENT_PROMPT},
            {"role": "user", "content": query}
        ]

        for chunk in self.call_llm_api_stream(messages):
            yield chunk